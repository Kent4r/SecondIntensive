{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ноутбук для расписи всего алгоритма обработки данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import missingno as msn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем дупликаты и полностью пустые колонки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='all', axis=1, inplace=True)\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим все возможные колонки в INT64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_columns = df.select_dtypes(include=np.number).columns\n",
    "for col in float_columns:\n",
    "    temp = df[col].fillna(0)\n",
    "    if temp.apply(lambda x: x == int(x)).all():\n",
    "        df[col] = df[col].astype(\"Int64\")\n",
    "df = df.loc[:, (df != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем датафрейм по типам данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_float = df.select_dtypes(include=[\"float64\"])\n",
    "df_int = df.select_dtypes(include=[\"int64\"])\n",
    "df_object = df.select_dtypes(include=[\"object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge'им каждый датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_object = df_object.astype(str)\n",
    "df_object.replace('0', '', inplace=True)\n",
    "df_object.replace('nan', np.nan, inplace=True)\n",
    "\n",
    "def merge_columns_with_common_values(df):\n",
    "    global count\n",
    "    count = 0\n",
    "    columns = df.columns.tolist()\n",
    "    merged_columns = []\n",
    "    i = 0\n",
    "    while i < len(columns) - 1:\n",
    "        current_col = columns[i]\n",
    "        next_col = columns[i + 1]\n",
    "        \n",
    "        common_values = list(set(df[current_col].unique()).intersection(set(df[next_col].unique())))\n",
    "        x = np.nan\n",
    "        if(x in common_values): common_values.remove(x)\n",
    "        if common_values:\n",
    "            count += 1\n",
    "            merged_name = f\"{current_col}\"\n",
    "            df[merged_name] = df[current_col].combine_first(df[next_col])\n",
    "            merged_columns.append(merged_name)\n",
    "            i += 2  \n",
    "        else:\n",
    "            merged_columns.append(current_col)\n",
    "            i += 1\n",
    "    \n",
    "    if i == len(columns) - 1:\n",
    "        merged_columns.append(columns[-1])\n",
    "    \n",
    "    df = df[merged_columns]\n",
    "    return df\n",
    "\n",
    "df_object = merge_columns_with_common_values(df_object)\n",
    "i = 1\n",
    "while count != 0:\n",
    "    i += 1\n",
    "    df_object = merge_columns_with_common_values(df_object)\n",
    "    print(f\"Iteration {i}: {len(df_object.columns)} columns left\")\n",
    "\n",
    "if count == 0:\n",
    "    print(\"Nothing changed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns_with_common_values(df):\n",
    "    base_toggle = True\n",
    "    count = 0\n",
    "    unique_columns = []\n",
    "    temp_col = \"col4\"\n",
    "    \n",
    "    df.replace([\"nan\", np.float64(0.0), np.float64(1.0)], np.nan, inplace=True)\n",
    "    \n",
    "    column_groups = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in unique_columns:  continue\n",
    "        \n",
    "        if col != df.columns[1]: base_toggle = False\n",
    "\n",
    "        column_groups[col] = [col]\n",
    "        \n",
    "        for next_col in df.columns:\n",
    "            if col == next_col or next_col in unique_columns: continue\n",
    "            \n",
    "            common_values = list(set(df[col].unique()).intersection(set(df[next_col].unique())))\n",
    "            if np.nan in common_values: common_values.remove(np.nan)\n",
    "            \n",
    "            match = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", col)\n",
    "            match1 = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", next_col)\n",
    "            def calc_jump():\n",
    "                if base_toggle == True: return False\n",
    "                else:\n",
    "                    match = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", temp_col)\n",
    "                    is_jump = ((int(match1.group(2)) - int(match.group(2))) > 5)\n",
    "                    return is_jump\n",
    "\n",
    "            if len(common_values) >= 1:\n",
    "                if calc_jump():\n",
    "                    temp_col = next_col\n",
    "                    break\n",
    "                count += 1\n",
    "                temp_col = next_col\n",
    "                column_groups[col].append(next_col)\n",
    "                unique_columns.append(next_col)\n",
    "                print(f\"Merged {col} and {next_col}: common_values = {common_values}\")\n",
    "            else:\n",
    "                if calc_jump():\n",
    "                    temp_col = next_col\n",
    "                    break\n",
    "                common_values = list(set(df[temp_col].unique()).intersection(set(df[next_col].unique())))\n",
    "                if len(common_values) >= 1:\n",
    "                    count += 1\n",
    "                    temp_col = next_col\n",
    "                    column_groups[col].append(next_col) \n",
    "                    unique_columns.append(next_col) \n",
    "                    print(f\"Merged {col} and {next_col}: common_values = {common_values}\")\n",
    "                else:break\n",
    "\n",
    "        unique_columns.append(col)\n",
    "    \n",
    "    for group_name, group_columns in column_groups.items():\n",
    "        if len(group_columns) > 1:\n",
    "            df[group_name] = df[group_columns].bfill(axis=1).iloc[:, 0]\n",
    "            print(f\"Merged columns {group_columns} into {group_name}\")\n",
    "        else:\n",
    "            df[group_name] = df[group_columns[0]]\n",
    "    \n",
    "    df = df[[col for col in df.columns if col in column_groups]]\n",
    "    \n",
    "    return df, count\n",
    "\n",
    "df_float, count = merge_columns_with_common_values(df_float)\n",
    "\n",
    "if count == 0:\n",
    "    print(\"Nothing changed!\")\n",
    "else:\n",
    "    print(f\"{count} columns were merged.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df_int.astype(float)\n",
    "\n",
    "df_int = df_int.drop([\"client_id\",\"target\"],axis=1)\n",
    "df_int.replace(\"nan\",np.nan, inplace=True)\n",
    "\n",
    "def merge_columns_with_common_values(df):\n",
    "    base_toggle = True\n",
    "    count = 0\n",
    "    unique_columns = []\n",
    "    temp_col = \"col2\"\n",
    "\n",
    "    column_groups = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in unique_columns: continue\n",
    "\n",
    "        if col != df.columns[1]: base_toggle = False\n",
    "\n",
    "        column_groups[col] = [col]\n",
    "\n",
    "        for next_col in df.columns:\n",
    "            if col == next_col or next_col in unique_columns:\n",
    "                continue\n",
    "\n",
    "            common_values = list(set(df[col].unique()).intersection(set(df[next_col].unique())))\n",
    "            if np.nan in common_values:\n",
    "                common_values.remove(np.nan)\n",
    "\n",
    "            match = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", col)\n",
    "            match1 = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", next_col)\n",
    "            def calc_jump():\n",
    "                if base_toggle:return False\n",
    "                else:\n",
    "                    match = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", temp_col)\n",
    "                    is_jump = (int(match1.group(2)) - int(match.group(2))) > 5\n",
    "                    return is_jump\n",
    "\n",
    "            if (len(common_values) >= 1):\n",
    "                if calc_jump():\n",
    "                    temp_col = next_col\n",
    "                    break\n",
    "                count += 1\n",
    "                temp_col = next_col\n",
    "                column_groups[col].append(next_col)\n",
    "                unique_columns.append(next_col)\n",
    "                print(f\"Merged {col} and {next_col}: common_values = {common_values}\")\n",
    "            else:\n",
    "                if calc_jump():\n",
    "                    temp_col = next_col\n",
    "                    break\n",
    "                common_values = list(\n",
    "                    set(df[temp_col].unique()).intersection(set(df[next_col].unique()))\n",
    "                )\n",
    "                if len(common_values) >= 1:\n",
    "                    count += 1\n",
    "                    temp_col = next_col\n",
    "                    column_groups[col].append(next_col)\n",
    "                    unique_columns.append(next_col)\n",
    "                    print(f\"Merged {col} and {next_col}: common_values = {common_values}\")\n",
    "                else:break\n",
    "\n",
    "        unique_columns.append(col)\n",
    "\n",
    "    for group_name, group_columns in column_groups.items():\n",
    "        if len(group_columns) > 1:\n",
    "            df[group_name] = (df[group_columns].bfill(axis=1).iloc[:, 0])\n",
    "            print(f\"Merged columns {group_columns} into {group_name}\")\n",
    "        else:\n",
    "            df[group_name] = df[group_columns[0]]\n",
    "\n",
    "    df = df[[col for col in df.columns if col in column_groups]]\n",
    "\n",
    "    return df, count\n",
    "\n",
    "df_int, count = merge_columns_with_common_values(df_int)\n",
    "\n",
    "if count == 0: print(\"Nothing changed!\")\n",
    "else: print(f\"{count} columns were merged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединяем обработаные фреймы в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_sorted = pd.concat([df_object['report_date'],df['client_id'], df['target']],axis=1)\n",
    "df_object.drop(columns='report_date', inplace=True)\n",
    "\n",
    "df_not_sorted = pd.concat([df_not_sorted, df_object, df_int, df_float],axis=1)\n",
    "df_not_sorted.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сортируем колонки во фрейме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dict = {}\n",
    "order_list = ['report_date','client_id','target']\n",
    "columns = df_not_sorted.columns\n",
    "for column in columns:\n",
    "    if 'col' in column:\n",
    "        order_dict[column] = int(re.match(r\"([a-zA-Z]+)(\\d{1,4})\", column).group(2))\n",
    "\n",
    "sorted_columns = {k: v for k, v in sorted(order_dict.items(), key=lambda item: item[1])}\n",
    "for k in sorted_columns:\n",
    "    order_list.append(k)\n",
    "\n",
    "df = df_not_sorted[order_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем столбцы со ссылками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_link(column): return column.astype(str).str.startswith(('http://', 'https://'), na=False).any()\n",
    "\n",
    "df = df.loc[:, ~df.apply(has_link)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим report_date в datetime и удаляем все столбцы object, тк оказывается с ними не очень удобно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['report_date'] = pd.to_datetime(df['report_date'], format='%Y-%m-%d')\n",
    "df = df.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом наша обработка закончена, сохраняем файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"processed_valid.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
