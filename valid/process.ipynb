{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ноутбук для расписи всего алгоритма обработки данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import missingno as msn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egorb\\AppData\\Local\\Temp\\ipykernel_16036\\1437252748.py:1: DtypeWarning: Columns (51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,131,132,133,135,136,139,140,141,142,143,144,145,146,147,148,149,151,152,155,156,157,158,159,160,161,163,164,165,167,168,169,171,172,173,174,175,176,177,178,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,223,224,227,239,240,241,242,243,244,245,246,459,460,461,467,468,469,515,516,523,524,531,532,539,540,547,548,555,556,563,564,565,567,571,572,573,575,579,580,581,582,583,587,588,595,596,603,604,611,612,613,615,795,796,797,799,1031,1032,1033,1034,1035,1036,1037,1039,1040,1041,1042,1043,1044,1045,1047,1048,1049,1050,1051,1052,1055,1056,1057,1058,1059,1060,1063,1064,1065,1067,1068,1069,1072,1073,1074,1075,1076,1077,1078,1172,1173,1174,1176,1177,1178,1179,1180,1181,1182,1275,1276,1277,1278,1279,1280,1281,1282,1447,1448,1449,1450,1451,1452,1453,1454,1649,1650,1651,1652,1653,1654,1655,1656,2193,2194,2195,2196,2199) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../train.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем дупликаты и полностью пустые колонки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='all', axis=1, inplace=True)\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим все возможные колонки в INT64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_columns = df.select_dtypes(include=np.number).columns\n",
    "for col in float_columns:\n",
    "    temp = df[col].fillna(0)\n",
    "    if temp.apply(lambda x: x == int(x)).all():\n",
    "        df[col] = df[col].astype(\"Int64\")\n",
    "df = df.loc[:, (df != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем датафрейм по типам данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_float = df.select_dtypes(include=[\"float64\"])\n",
    "df_int = df.select_dtypes(include=[\"int64\"])\n",
    "df_object = df.select_dtypes(include=[\"object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge'им каждый датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2: 89 columns left\n",
      "Iteration 3: 46 columns left\n",
      "Iteration 4: 39 columns left\n",
      "Iteration 5: 37 columns left\n",
      "Iteration 6: 37 columns left\n",
      "Nothing changed!\n"
     ]
    }
   ],
   "source": [
    "df_object = df_object.astype(str)\n",
    "df_object.replace('0', '', inplace=True)\n",
    "df_object.replace('nan', np.nan, inplace=True)\n",
    "\n",
    "def merge_columns_with_common_values(df):\n",
    "    global count\n",
    "    count = 0\n",
    "    columns = df.columns.tolist()\n",
    "    merged_columns = []\n",
    "    i = 0\n",
    "    while i < len(columns) - 1:\n",
    "        current_col = columns[i]\n",
    "        next_col = columns[i + 1]\n",
    "        \n",
    "        common_values = list(set(df[current_col].unique()).intersection(set(df[next_col].unique())))\n",
    "        x = np.nan\n",
    "        if(x in common_values): common_values.remove(x)\n",
    "        if common_values:\n",
    "            count += 1\n",
    "            merged_name = f\"{current_col}\"\n",
    "            df[merged_name] = df[current_col].combine_first(df[next_col])\n",
    "            merged_columns.append(merged_name)\n",
    "            i += 2  \n",
    "        else:\n",
    "            merged_columns.append(current_col)\n",
    "            i += 1\n",
    "    \n",
    "    if i == len(columns) - 1:\n",
    "        merged_columns.append(columns[-1])\n",
    "    \n",
    "    df = df[merged_columns]\n",
    "    return df\n",
    "\n",
    "df_object = merge_columns_with_common_values(df_object)\n",
    "i = 1\n",
    "while count != 0:\n",
    "    i += 1\n",
    "    df_object = merge_columns_with_common_values(df_object)\n",
    "    print(f\"Iteration {i}: {len(df_object.columns)} columns left\")\n",
    "\n",
    "if count == 0:\n",
    "    print(\"Nothing changed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns_with_common_values(df):\n",
    "    base_toggle = True\n",
    "    count = 0\n",
    "    unique_columns = []\n",
    "    temp_col = \"col4\"\n",
    "    \n",
    "    df.replace([\"nan\", np.float64(0.0), np.float64(1.0)], np.nan, inplace=True)\n",
    "    \n",
    "    column_groups = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in unique_columns:  continue\n",
    "        \n",
    "        if col != df.columns[1]: base_toggle = False\n",
    "\n",
    "        column_groups[col] = [col]\n",
    "        \n",
    "        for next_col in df.columns:\n",
    "            if col == next_col or next_col in unique_columns: continue\n",
    "            \n",
    "            common_values = list(set(df[col].unique()).intersection(set(df[next_col].unique())))\n",
    "            if np.nan in common_values: common_values.remove(np.nan)\n",
    "            \n",
    "            match = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", col)\n",
    "            match1 = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", next_col)\n",
    "            def calc_jump():\n",
    "                if base_toggle == True: return False\n",
    "                else:\n",
    "                    match = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", temp_col)\n",
    "                    is_jump = ((int(match1.group(2)) - int(match.group(2))) > 5)\n",
    "                    return is_jump\n",
    "\n",
    "            if len(common_values) >= 1:\n",
    "                if calc_jump():\n",
    "                    temp_col = next_col\n",
    "                    break\n",
    "                count += 1\n",
    "                temp_col = next_col\n",
    "                column_groups[col].append(next_col)\n",
    "                unique_columns.append(next_col)\n",
    "                print(f\"Merged {col} and {next_col}: common_values = {common_values}\")\n",
    "            else:\n",
    "                if calc_jump():\n",
    "                    temp_col = next_col\n",
    "                    break\n",
    "                common_values = list(set(df[temp_col].unique()).intersection(set(df[next_col].unique())))\n",
    "                if len(common_values) >= 1:\n",
    "                    count += 1\n",
    "                    temp_col = next_col\n",
    "                    column_groups[col].append(next_col) \n",
    "                    unique_columns.append(next_col) \n",
    "                    print(f\"Merged {col} and {next_col}: common_values = {common_values}\")\n",
    "                else:break\n",
    "\n",
    "        unique_columns.append(col)\n",
    "    \n",
    "    for group_name, group_columns in column_groups.items():\n",
    "        if len(group_columns) > 1:\n",
    "            df[group_name] = df[group_columns].bfill(axis=1).iloc[:, 0]\n",
    "            print(f\"Merged columns {group_columns} into {group_name}\")\n",
    "        else:\n",
    "            df[group_name] = df[group_columns[0]]\n",
    "    \n",
    "    df = df[[col for col in df.columns if col in column_groups]]\n",
    "    \n",
    "    return df, count\n",
    "\n",
    "df_float, count = merge_columns_with_common_values(df_float)\n",
    "\n",
    "if count == 0:\n",
    "    print(\"Nothing changed!\")\n",
    "else:\n",
    "    print(f\"{count} columns were merged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int = df_int.astype(float)\n",
    "\n",
    "df_int = df_int.drop([\"client_id\",\"target\"],axis=1)\n",
    "df_int.replace(\"nan\",np.nan, inplace=True)\n",
    "\n",
    "def merge_columns_with_common_values(df):\n",
    "    base_toggle = True\n",
    "    count = 0\n",
    "    unique_columns = []\n",
    "    temp_col = \"col2\"\n",
    "\n",
    "    column_groups = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in unique_columns: continue\n",
    "\n",
    "        if col != df.columns[1]: base_toggle = False\n",
    "\n",
    "        column_groups[col] = [col]\n",
    "\n",
    "        for next_col in df.columns:\n",
    "            if col == next_col or next_col in unique_columns:\n",
    "                continue\n",
    "\n",
    "            common_values = list(set(df[col].unique()).intersection(set(df[next_col].unique())))\n",
    "            if np.nan in common_values:\n",
    "                common_values.remove(np.nan)\n",
    "\n",
    "            match = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", col)\n",
    "            match1 = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", next_col)\n",
    "            def calc_jump():\n",
    "                if base_toggle:return False\n",
    "                else:\n",
    "                    match = re.match(r\"([a-zA-Z]+)(\\d{1,4})\", temp_col)\n",
    "                    is_jump = (int(match1.group(2)) - int(match.group(2))) > 5\n",
    "                    return is_jump\n",
    "\n",
    "            if (len(common_values) >= 1):\n",
    "                if calc_jump():\n",
    "                    temp_col = next_col\n",
    "                    break\n",
    "                count += 1\n",
    "                temp_col = next_col\n",
    "                column_groups[col].append(next_col)\n",
    "                unique_columns.append(next_col)\n",
    "                print(f\"Merged {col} and {next_col}: common_values = {common_values}\")\n",
    "            else:\n",
    "                if calc_jump():\n",
    "                    temp_col = next_col\n",
    "                    break\n",
    "                common_values = list(\n",
    "                    set(df[temp_col].unique()).intersection(set(df[next_col].unique()))\n",
    "                )\n",
    "                if len(common_values) >= 1:\n",
    "                    count += 1\n",
    "                    temp_col = next_col\n",
    "                    column_groups[col].append(next_col)\n",
    "                    unique_columns.append(next_col)\n",
    "                    print(f\"Merged {col} and {next_col}: common_values = {common_values}\")\n",
    "                else:break\n",
    "\n",
    "        unique_columns.append(col)\n",
    "\n",
    "    for group_name, group_columns in column_groups.items():\n",
    "        if len(group_columns) > 1:\n",
    "            df[group_name] = (df[group_columns].bfill(axis=1).iloc[:, 0])\n",
    "            print(f\"Merged columns {group_columns} into {group_name}\")\n",
    "        else:\n",
    "            df[group_name] = df[group_columns[0]]\n",
    "\n",
    "    df = df[[col for col in df.columns if col in column_groups]]\n",
    "\n",
    "    return df, count\n",
    "\n",
    "df_int, count = merge_columns_with_common_values(df_int)\n",
    "\n",
    "if count == 0: print(\"Nothing changed!\")\n",
    "else: print(f\"{count} columns were merged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединяем обработаные фреймы в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14456 entries, 0 to 14455\n",
      "Columns: 168 entries, report_date to col2663\n",
      "dtypes: Int64(2), float64(129), object(37)\n",
      "memory usage: 18.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_not_sorted = pd.concat([df_object['report_date'],df['client_id'], df['target']],axis=1)\n",
    "df_object.drop(columns='report_date', inplace=True)\n",
    "\n",
    "df_not_sorted = pd.concat([df_not_sorted, df_object, df_int, df_float],axis=1)\n",
    "df_not_sorted.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сортируем колонки во фрейме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dict = {}\n",
    "order_list = ['report_date','client_id','target']\n",
    "columns = df_not_sorted.columns\n",
    "for column in columns:\n",
    "    if 'col' in column:\n",
    "        order_dict[column] = int(re.match(r\"([a-zA-Z]+)(\\d{1,4})\", column).group(2))\n",
    "\n",
    "sorted_columns = {k: v for k, v in sorted(order_dict.items(), key=lambda item: item[1])}\n",
    "for k in sorted_columns:\n",
    "    order_list.append(k)\n",
    "\n",
    "df = df_not_sorted[order_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем столбцы со ссылками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_link(column): return column.astype(str).str.startswith(('http://', 'https://'), na=False).any()\n",
    "\n",
    "df = df.loc[:, ~df.apply(has_link)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим report_date в datetime и удаляем все столбцы object, тк оказывается с ними не очень удобно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['report_date'] = pd.to_datetime(df['report_date'], format='%Y-%m-%d')\n",
    "df = df.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом наша обработка закончена, сохраняем файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../processed_train.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
